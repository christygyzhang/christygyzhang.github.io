<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Projects | Christy (Guanyi) Zhang</title>
  <link rel="stylesheet" href="assets/css/main.css">
  <div class="container">
    <nav>
    <a href="index.html">Home</a> |
    <a href="projects.html">Projects</a>
    </nav>
</div>
</head>
<body>
  <div class="container">
    <h1>My Projects</h1>
    
    <div class="project">
      <h2>Resource-Efficient Large Language Model for Log Anomaly Detection</h2>
      <p>
        This project explores how to make large language models more practical for log anomaly detection in resource-limited environments.
        I developed a two-stage optimization pipeline combining Quantized Low-Rank Adaptation (QLoRA) for memory-efficient fine-tuning
        and iterative structured pruning to compress the model for deployment.
        Using the HDFS dataset, the pipeline reduced training memory by 85% and cut inference latency by 45%
        while maintaining or improving accuracy (F1 = 0.944).
        The optimized model was successfully deployed on a Raspberry Pi, demonstrating the feasibility of running transformer-based
        log analytics on edge devices.
      </p>
      <p>
        <a class="report-link" href="assets/project-reports/efficient-llm-lad.pdf" target="_blank">ðŸ“„ View Report</a>
      </p>
    </div>

    <div class="project">
      <h2>Investigating the Role of Test Cases in LLM-Based Python and JavaScript Code Translation</h2>
      <p>
        This project investigates how incorporating test cases can improve the accuracy of code translation between Python and JavaScript.
        We developed a structured prompting pipeline that progressively enhances translation quality.
        Starting from a simple instruction-based baseline, we first introduced test cases into the prompt to provide execution-level context, then added a one-shot example to guide consistent translation behavior.
        Beyond these prompt variations, we applied an iterative refinement process that allows the language model to reattempt failed translations using feedback from previous outputs.
        Translation correctness was evaluated by running the translated code against ground-truth test cases from the HumanEval-X dataset.
        Experiments with Gemini-2.0-Flash and GPT-3.5-Turbo show that these techniques substantially improved translation accuracyâ€”from 87.8% to 100% for Gemini and from 44.3% to over 90% for GPT-3.5â€”demonstrating the effectiveness of test-aware prompting and refinement in improving code reliability.
      </p>
      <p>
        <a class="report-link" href="assets/project-reports/llm-code-translation.pdf" target="_blank">ðŸ“„ View Report</a>
      </p>
    </div>

    <div class="project">
      <h2>Investigating Gender Bias in COMET-ATOMIC<span class="atomic20"><span>20</span><span>20</span></span></h2>
      <p>
        This project examines whether generative commonsense reasoning models encode gender bias in their outputs.
        We analyzed COMET-ATOMIC<span class="atomic20"><span>20</span><span>20</span></span>, a model that generates if-then commonsense inferences, by systematically varying only the gendered subject names in event prompts.
        Using a controlled dataset adapted from the WinoBias benchmark and curated lists of male, female, and unisex names, we compared outputs across BART- and GPT-2-based COMET-ATOMIC<span class="atomic20"><span>20</span><span>20</span></span> models.
        The analysis included sentiment scoring, agreement evaluation, and lexical bias detection.
        Results revealed statistically significant gender-based differences: for example, female-associated outputs contained more family-related terms and emotional expressions, while male-associated ones showed stronger associations with social and power-related language.
        These findings highlight that even advanced generative commonsense models can reproduce or amplify gendered patterns, underscoring the need for bias evaluation in reasoning-level AI systems.
      </p>
      <p>
        <a class="report-link" href="assets/pdfs/bias-comet-atomic.pdf" target="_blank">ðŸ“„ View Final Report</a>
      </p>
    </div>
    

    <div class="project">
      <h2>Transformer-Based Actor-Critic Agent</h2>
      <p>
        This project explores how transformer architectures can enhance reinforcement learning by enabling agents to better capture long-term dependencies and sequential decision patterns. 
        We designed a Transformer-Based Actor-Critic (TBAC) model that integrates transformer blocks into the critic network, leveraging self-attention to model temporal relationships in stateâ€“action trajectories.
        The model was evaluated on OpenAI Gymâ€™s Pendulum environment, where it demonstrated improved stability and generalization compared to a standard Actor-Critic (AC) baseline.
      </p>
      <p>
        <a class="report-link" href="assets/pdfs/transformer-RL.pdf" target="_blank">ðŸ“„ View Final Report</a>
      </p>
    </div>

  </div>
</body>
</html>
